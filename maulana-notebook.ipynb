{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from modules import components, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"data\"\n",
    "OUTPUT_BASE = \"maulanaa_agss-pipeline\"\n",
    "\n",
    "PIPELINE_NAME = \"mental-health-twitter-pipeline\"\n",
    "\n",
    "TRANSFORM_MODULE_PATH = \"modules/transform.py\"\n",
    "TUNER_MODULE_PATH = \"modules/tuner.py\"\n",
    "TRAINER_MODULE_PATH = \"modules/trainer.py\"\n",
    "\n",
    "SERVING_DIRECTORY = \"serving_model_dir/mental-health-twitter-model\"\n",
    "\n",
    "PIPELINE_ROOT = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "METADATA_PATH = os.path.join(PIPELINE_ROOT, \"metadata.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 10m 36s]\n",
      "binary_accuracy: 0.745312511920929\n",
      "\n",
      "Best binary_accuracy So Far: 0.879687488079071\n",
      "Total elapsed time: 00h 56m 38s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Tuner\\.system\\executor_execution\\7\\.temp\\7\\kt_hyperband\n",
      "Showing 10 best trials\n",
      "Objective(name=\"binary_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0006 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "embed_dims: 16\n",
      "lstm_units: 64\n",
      "dense_units: 160\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0002\n",
      "Score: 0.879687488079071\n",
      "\n",
      "Trial 0005 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "embed_dims: 48\n",
      "lstm_units: 128\n",
      "dense_units: 96\n",
      "dropout_rate: 0.5\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0004\n",
      "Score: 0.8492187261581421\n",
      "\n",
      "Trial 0004 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "embed_dims: 48\n",
      "lstm_units: 128\n",
      "dense_units: 96\n",
      "dropout_rate: 0.5\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.768750011920929\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "embed_dims: 16\n",
      "lstm_units: 64\n",
      "dense_units: 160\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.7601562738418579\n",
      "\n",
      "Trial 0009 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "embed_dims: 48\n",
      "lstm_units: 96\n",
      "dense_units: 32\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.745312511920929\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "embed_dims: 16\n",
      "lstm_units: 32\n",
      "dense_units: 64\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.731249988079071\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "embed_dims: 48\n",
      "lstm_units: 64\n",
      "dense_units: 128\n",
      "dropout_rate: 0.4\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.7164062261581421\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "embed_dims: 48\n",
      "lstm_units: 128\n",
      "dense_units: 256\n",
      "dropout_rate: 0.4\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.703906238079071\n",
      "\n",
      "Trial 0008 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "embed_dims: 48\n",
      "lstm_units: 96\n",
      "dense_units: 128\n",
      "dropout_rate: 0.5\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.68359375\n",
      "\n",
      "Trial 0007 summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "embed_dims: 48\n",
      "lstm_units: 128\n",
      "dense_units: 160\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.678906261920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " post_text_xf (InputLayer)   [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 500)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 500, 16)           80000     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              41472     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 160)               20640     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 142,273\n",
      "Trainable params: 142,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.3877 - binary_accuracy: 0.8153\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.78552, saving model to maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 310s 1s/step - loss: 0.3877 - binary_accuracy: 0.8153 - val_loss: 0.4145 - val_binary_accuracy: 0.7855\n",
      "Epoch 2/5\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.2215 - binary_accuracy: 0.8982\n",
      "Epoch 2: val_binary_accuracy improved from 0.78552 to 0.81828, saving model to maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 381s 1s/step - loss: 0.2215 - binary_accuracy: 0.8982 - val_loss: 0.3938 - val_binary_accuracy: 0.8183\n",
      "Epoch 3/5\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.1598 - binary_accuracy: 0.9271\n",
      "Epoch 3: val_binary_accuracy improved from 0.81828 to 0.82229, saving model to maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 334s 1s/step - loss: 0.1598 - binary_accuracy: 0.9271 - val_loss: 0.4621 - val_binary_accuracy: 0.8223\n",
      "Epoch 4/5\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.1342 - binary_accuracy: 0.9395\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.82229\n",
      "300/300 [==============================] - 324s 1s/step - loss: 0.1342 - binary_accuracy: 0.9395 - val_loss: 0.5557 - val_binary_accuracy: 0.8080\n",
      "Epoch 5/5\n",
      " 48/300 [===>..........................] - ETA: 2:57 - loss: 0.1163 - binary_accuracy: 0.9459WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1500 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1500 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_binary_accuracy did not improve from 0.82229\n",
      "300/300 [==============================] - 149s 495ms/step - loss: 0.1163 - binary_accuracy: 0.9459 - val_loss: 0.5278 - val_binary_accuracy: 0.8203\n",
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: maulanaa_agss-pipeline\\mental-health-twitter-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000016E1F611DC0> and <keras.engine.input_layer.InputLayer object at 0x0000016E4C029580>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000016E1F611DC0> and <keras.engine.input_layer.InputLayer object at 0x0000016E4C029580>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000016E316745B0> and <keras.engine.input_layer.InputLayer object at 0x0000016E31892E50>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000016E316745B0> and <keras.engine.input_layer.InputLayer object at 0x0000016E31892E50>).\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684673   nanos: 870526552 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684673   nanos: 875705718 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684690   nanos: 997542142 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C5CEFCA640> and <keras.engine.input_layer.InputLayer object at 0x000001C5CEB13580>).\" instruction_id: \"bundle_166\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684699   nanos: 822762012 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C5E3A358E0> and <keras.engine.input_layer.InputLayer object at 0x000001C5E3A14EB0>).\" instruction_id: \"bundle_166\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684700   nanos: 744486093 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_166\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684718   nanos: 286254405 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C5F6704880> and <keras.engine.input_layer.InputLayer object at 0x000001C5F677B250>).\" instruction_id: \"bundle_172\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684727   nanos: 89664459 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C5882761F0> and <keras.engine.input_layer.InputLayer object at 0x000001C5870E7AF0>).\" instruction_id: \"bundle_172\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684736   nanos: 34896373 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C609F6ABE0> and <keras.engine.input_layer.InputLayer object at 0x000001C609EEF850>).\" instruction_id: \"bundle_175\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684744   nanos: 850146770 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C61A994850> and <keras.engine.input_layer.InputLayer object at 0x000001C61AAA2040>).\" instruction_id: \"bundle_178\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1713684753   nanos: 738760232 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001C62C7565E0> and <keras.engine.input_layer.InputLayer object at 0x000001C62C5AC940>).\" instruction_id: \"bundle_178\" log_location: \"E:\\\\Software-Kuliah\\\\anaconda3\\\\envs\\\\final\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-14\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Software-Kuliah\\anaconda3\\envs\\final\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Software-Kuliah\\anaconda3\\envs\\final\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
     ]
    }
   ],
   "source": [
    "components_args = {\n",
    "    \"data_root\": DATA_ROOT,\n",
    "    \"transform_module_path\": TRANSFORM_MODULE_PATH,\n",
    "    \"tuner_module_path\": TUNER_MODULE_PATH,\n",
    "    \"trainer_module_path\": TRAINER_MODULE_PATH,\n",
    "    \"train_steps\": 300,\n",
    "    \"eval_steps\": 300,\n",
    "    \"serving_model_dir\": SERVING_DIRECTORY,\n",
    "}\n",
    "\n",
    "components = components.init_components(components_args)\n",
    "\n",
    "pipeline = pipeline.init_pipeline(\n",
    "    PIPELINE_ROOT,\n",
    "    PIPELINE_NAME,\n",
    "    METADATA_PATH,\n",
    "    components,\n",
    ")\n",
    "\n",
    "BeamDagRunner().run(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
